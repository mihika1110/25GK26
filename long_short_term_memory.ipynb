{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b31e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM with activations: LSTM(tanh as an Activation Function AND sigmoid as a Recurrent Activation Function) + Dense(relu as an Activation Function for the Dense Layer)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0296 - val_loss: 0.0192\n",
      "Epoch 2/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0196 - val_loss: 0.0184\n",
      "Epoch 3/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0188 - val_loss: 0.0177\n",
      "Epoch 4/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0182 - val_loss: 0.0177\n",
      "Epoch 5/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0177 - val_loss: 0.0174\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "\n",
      "Training LSTM with activations: LSTM(relu as an Activation Function AND sigmoid as a Recurrent Activation Function) + Dense(relu as an Activation Function for the Dense Layer)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0340 - val_loss: 0.0192\n",
      "Epoch 2/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0200 - val_loss: 0.0180\n",
      "Epoch 3/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0190 - val_loss: 0.0173\n",
      "Epoch 4/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0183 - val_loss: 0.0169\n",
      "Epoch 5/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0177 - val_loss: 0.0166\n",
      "Epoch 6/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0172 - val_loss: 0.0164\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "\n",
      "Training LSTM with activations: LSTM(selu as an Activation Function AND tanh as a Recurrent Activation Function) + Dense(relu as an Activation Function for the Dense Layer)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0339 - val_loss: 0.0194\n",
      "Epoch 2/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0194 - val_loss: 0.0180\n",
      "Epoch 3/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0183 - val_loss: 0.0170\n",
      "Epoch 4/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0175 - val_loss: 0.0165\n",
      "Epoch 5/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0170 - val_loss: 0.0163\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "Training LSTM with activations: LSTM(tanh as an Activation Function AND hard_sigmoid as a Recurrent Activation Function) + Dense(elu as an Activation Function for the Dense Layer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0299 - val_loss: 0.0195\n",
      "Epoch 2/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0205 - val_loss: 0.0189\n",
      "Epoch 3/20\n",
      "\u001b[1m341/341\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0198 - val_loss: 0.0189\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "\n",
      "~~~~~ Activation Function Comparison ~~~~~\n",
      "  LSTM_Activation Recurrent_Activation Dense_Activation  PV_R2  Wind_R2  \\\n",
      "1            relu              sigmoid             relu  0.921    0.535   \n",
      "2            selu                 tanh             relu  0.916    0.542   \n",
      "0            tanh              sigmoid             relu  0.911    0.527   \n",
      "3            tanh         hard_sigmoid              elu  0.902    0.478   \n",
      "\n",
      "   PV_Adj_R2  Wind_Adj_R2  PV_MSE  Wind_MSE  PV_RMSE  Wind_RMSE  PV_MAE  \\\n",
      "1      0.921        0.535   0.008     0.024    0.092      0.156   0.054   \n",
      "2      0.916        0.541   0.009     0.024    0.095      0.155   0.060   \n",
      "0      0.911        0.527   0.010     0.025    0.097      0.158   0.062   \n",
      "3      0.902        0.477   0.010     0.027    0.102      0.165   0.068   \n",
      "\n",
      "   Wind_MAE  \n",
      "1     0.124  \n",
      "2     0.124  \n",
      "0     0.126  \n",
      "3     0.134  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Step 2: Load Data\n",
    "train_data = pd.read_csv(\"train_multi_output.csv\")\n",
    "test_data = pd.read_csv(\"test_multi_output.csv\")\n",
    "\n",
    "# Step 3: Select Features and Targets\n",
    "X_train = train_data[['Season', 'Day_of_the_week', 'DHI', 'DNI', 'GHI','Wind_speed', 'Humidity', 'Temperature']].values\n",
    "y_train = train_data[['PV_production', 'Wind_production']].values\n",
    "\n",
    "X_test = test_data[['Season', 'Day_of_the_week', 'DHI', 'DNI', 'GHI','Wind_speed', 'Humidity', 'Temperature']].values\n",
    "y_test = test_data[['PV_production', 'Wind_production']].values\n",
    "\n",
    "# Step 4: Scale the data (LSTM performs better with scaled inputs)\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "# Step 5: Reshape input for LSTM [samples, timesteps, features]\n",
    "X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ============================================================\n",
    "# âš™ï¸ Step 6: Build LSTM Model with Multiple Activation Functions\n",
    "# ============================================================\n",
    "\n",
    "def build_lstm_model(activation_fn='tanh', recurrent_activation_fn='sigmoid', dense_activation='relu'):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # LSTM Layer with customizable activations\n",
    "    model.add(LSTM(units=64, \n",
    "                   activation=activation_fn, \n",
    "                   recurrent_activation=recurrent_activation_fn,\n",
    "                   input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]),\n",
    "                   return_sequences=False))\n",
    "    \n",
    "    # Dropout to prevent overfitting\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Dense Layer with chosen activation\n",
    "    model.add(Dense(32, activation=dense_activation))\n",
    "    \n",
    "    # Output Layer - 2 neurons for PV and Wind production (linear activation for regression)\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Try different combinations of activations\n",
    "activation_combos = [\n",
    "    ('tanh', 'sigmoid', 'relu'),\n",
    "    ('relu', 'sigmoid', 'relu'),\n",
    "    ('selu', 'tanh', 'relu'),\n",
    "    ('tanh', 'hard_sigmoid', 'elu')\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# ============================================================\n",
    "# ğŸš€ Step 7: Train & Evaluate Different Activation Combinations\n",
    "# ============================================================\n",
    "for act_fn, rec_act_fn, dense_act_fn in activation_combos:\n",
    "    print(f\"\\nTraining LSTM with activations: LSTM({act_fn} as an Activation Function AND {rec_act_fn} as a Recurrent Activation Function) + Dense({dense_act_fn} as an Activation Function for the Dense Layer)\")\n",
    "    \n",
    "    model = build_lstm_model(activation_fn=act_fn, \n",
    "                             recurrent_activation_fn=rec_act_fn, \n",
    "                             dense_activation=dense_act_fn)\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True, min_delta=0.001)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train_scaled,\n",
    "        epochs=20,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Predict and inverse transform\n",
    "    y_pred_scaled = model.predict(X_test_scaled)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    mae_pv = mean_absolute_error(y_test[:, 0], y_pred[:, 0])\n",
    "    mae_wind = mean_absolute_error(y_test[:, 1], y_pred[:, 1])\n",
    "    mse_pv = mean_squared_error(y_test[:, 0], y_pred[:, 0])\n",
    "    mse_wind = mean_squared_error(y_test[:, 1], y_pred[:, 1])\n",
    "    rmse_pv = np.sqrt(mse_pv)\n",
    "    rmse_wind = np.sqrt(mse_wind)\n",
    "    r2_pv = r2_score(y_test[:, 0], y_pred[:, 0])\n",
    "    r2_wind = r2_score(y_test[:, 1], y_pred[:, 1])\n",
    "    \n",
    "    # Adjusted RÂ² Calculation\n",
    "    n = y_test.shape[0]  # number of samples\n",
    "    p = X_test.shape[1]  # number of predictors (features)\n",
    "    \n",
    "    adj_r2_pv = 1 - ((1 - r2_pv) * (n - 1)) / (n - p - 1)\n",
    "    adj_r2_wind = 1 - ((1 - r2_wind) * (n - 1)) / (n - p - 1)\n",
    "    \n",
    "    results.append({\n",
    "        'LSTM_Activation': act_fn,\n",
    "        'Recurrent_Activation': rec_act_fn,\n",
    "        'Dense_Activation': dense_act_fn,\n",
    "        'PV_R2': r2_pv,\n",
    "        'Wind_R2': r2_wind,\n",
    "        'PV_Adj_R2': adj_r2_pv,\n",
    "        'Wind_Adj_R2': adj_r2_wind,\n",
    "        'PV_MSE': mse_pv,\n",
    "        'Wind_MSE': mse_wind,\n",
    "        'PV_RMSE': rmse_pv,\n",
    "        'Wind_RMSE': rmse_wind,\n",
    "        'PV_MAE': mae_pv,\n",
    "        'Wind_MAE': mae_wind\n",
    "    })\n",
    "\n",
    "# ============================================================\n",
    "# ğŸ“ŠStep 8: Compare Activation Function Combinations and Display in a Single Table\n",
    "# ============================================================\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the DataFrame as a single table\n",
    "print(\"\\n~~~~~ Activation Function Comparison ~~~~~\")\n",
    "print(results_df.sort_values(by=['PV_R2', 'Wind_R2'], ascending=False).round(3))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
